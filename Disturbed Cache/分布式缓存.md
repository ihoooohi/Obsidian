分布式缓存分为分布式和缓存
## 分布式

什么是分布式？

多个节点（服务器）完成同一个系统目标，并且节点之间需要通信

怎么通信？

通信有三层：
```text
【调用方式】   RPC / REST
【传输协议】   HTTP / TCP
【序列化】     JSON / Protobuf
```

## 缓存

### 1. 缓存算法/淘汰策略
#### LRU (Least Recently Used)

最近最没使用，时间维度
#### LFU (Least Frequently Used)

最不频繁使用，次数维度
#### FIFO (First in First out)
### 2. KLRU缓存的数据结构

哈希链表

链表能满足O(1)的插入和删除，哈希能满足O(1)的查找，Cache结构如下：

```go
type Cache struct {
	maxbytes int64
	nbytes int64
	ll *list.List
	cache map[string]*list.Element
	OnEvicted func(key string, val CacheValue)
}
```
`list.Element`中存放的不是一个数据CacheValue，而是一对（key，CacheValue），因为如果只存放CacheValue，淘汰节点时，无法反查对应的key，导致对应的key在map中删不掉，**这是反向索引的经典设计**

所以用一个新的struct来存list.Element的值叫entry：
```go
type entry struct {
	key string
	val CacheValue
}
```
CacheValue的接口设计：
```go
type CacheValue struct {
	Len() int
}
```
![[82c047ec3d8b178570b13410e40e6de3.jpg]]

### 3. 单机并发缓存的处理

**单机：** 一台服务器
**并发：** 一个进程内的多个执行单元（线程/协程）访问缓存

在go中，执行单元是协程goroutine，当多个goroutine同时读写缓存时，需要对共享状态进行并发控制（synchronization），否则会发生数据竞争（race condition）

常见处理方式：

1. **互斥与同步（sync 包）**
   - `sync.Mutex`：保护缓存的并发读写（写多或简单场景）
   - `sync.RWMutex`：读多写少场景，读并发、写互斥
   - `sync.Map`：适用于 key 稳定、读多写少的场景

2. **串行化访问（Channel / Actor 模式）**
   - 使用一个 goroutine 独占缓存
   - 其他 goroutine 通过 channel 发送 Get/Set 请求
   - 避免显式加锁，逻辑清晰

3. **回源合并（防缓存击穿）**
   - 使用 `singleflight` 合并同一 key 的并发回源请求
   - 确保只有一个 goroutine 执行加载并写缓存

那么LRU缓存中哪些需要控制并发呢？

Add(), Get(), 